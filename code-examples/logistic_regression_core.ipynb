{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhGuhbZ6M5tl"
   },
   "source": [
    "##### Copyright 2022 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-09-29T01:21:59.005443Z",
     "iopub.status.busy": "2022-09-29T01:21:59.004875Z",
     "iopub.status.idle": "2022-09-29T01:21:59.009530Z",
     "shell.execute_reply": "2022-09-29T01:21:59.008784Z"
    },
    "id": "AwOEIRJC6Une"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Logistic regression for binary classification with Core APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBIlTPscrIT9"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/logistic_regression_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/core/logistic_regression_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DauaqJ7WhIhO"
   },
   "source": [
    "This guide demonstrates how to use the [TensorFlow Core low-level APIs](https://www.tensorflow.org/guide/core) to perform [binary classification](https://developers.google.com/machine-learning/glossary#binary_classification){:.external} with [logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external}. It uses the [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} for tumor classification.\n",
    "\n",
    "[Logistic regression](https://developers.google.com/machine-learning/crash-course/logistic-regression/){:.external} is one of the most popular algorithms for binary classification. Given a set of examples with features, the goal of logistic regression is to output values between 0 and 1, which can be interpreted as the probabilities of each example belonging to a particular  class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nchsZfwEVtVs"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This tutorial uses [pandas](https://pandas.pydata.org){:.external} for reading a CSV file into a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html){:.external}, [seaborn](https://seaborn.pydata.org){:.external} for plotting a pairwise relationship in a dataset, [Scikit-learn](https://scikit-learn.org/){:.external} for computing a confusion matrix, and [matplotlib](https://matplotlib.org/){:.external} for creating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:21:59.014912Z",
     "iopub.status.busy": "2022-09-29T01:21:59.014352Z",
     "iopub.status.idle": "2022-09-29T01:22:00.678069Z",
     "shell.execute_reply": "2022-09-29T01:22:00.676921Z"
    },
    "id": "5lZoUK6AVTos"
   },
   "outputs": [],
   "source": [
    "# !pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:00.682987Z",
     "iopub.status.busy": "2022-09-29T01:22:00.682689Z",
     "iopub.status.idle": "2022-09-29T01:22:03.886255Z",
     "shell.execute_reply": "2022-09-29T01:22:03.885171Z"
    },
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Preset matplotlib figure sizes.\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
    "\n",
    "print(tf.__version__)\n",
    "# To make the results reproducible, set the random seed value.\n",
    "tf.random.set_seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Next, load the [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)){:.external} from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/){:.external}. This dataset contains various features such as a tumor's radius, texture, and concavity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:03.890936Z",
     "iopub.status.busy": "2022-09-29T01:22:03.890458Z",
     "iopub.status.idle": "2022-09-29T01:22:03.895827Z",
     "shell.execute_reply": "2022-09-29T01:22:03.894955Z"
    },
    "id": "CiX2FI4gZtTt"
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "\n",
    "features = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n",
    "            'concavity', 'concave_poinits', 'symmetry', 'fractal_dimension']\n",
    "column_names = ['id', 'diagnosis']\n",
    "\n",
    "for attr in ['mean', 'ste', 'largest']:\n",
    "  for feature in features:\n",
    "    column_names.append(feature + \"_\" + attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3VR1aTP92nV"
   },
   "source": [
    "Read the dataset into a pandas [DataFrame](){:.external} using [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html){:.external}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:03.900042Z",
     "iopub.status.busy": "2022-09-29T01:22:03.899721Z",
     "iopub.status.idle": "2022-09-29T01:22:04.325235Z",
     "shell.execute_reply": "2022-09-29T01:22:04.324382Z"
    },
    "id": "uvR2Bzb691lJ"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(url, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.329307Z",
     "iopub.status.busy": "2022-09-29T01:22:04.329047Z",
     "iopub.status.idle": "2022-09-29T01:22:04.343267Z",
     "shell.execute_reply": "2022-09-29T01:22:04.342349Z"
    },
    "id": "YB9eq6Zq-IZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         569 non-null    int64  \n",
      " 1   diagnosis                  569 non-null    object \n",
      " 2   radius_mean                569 non-null    float64\n",
      " 3   texture_mean               569 non-null    float64\n",
      " 4   perimeter_mean             569 non-null    float64\n",
      " 5   area_mean                  569 non-null    float64\n",
      " 6   smoothness_mean            569 non-null    float64\n",
      " 7   compactness_mean           569 non-null    float64\n",
      " 8   concavity_mean             569 non-null    float64\n",
      " 9   concave_poinits_mean       569 non-null    float64\n",
      " 10  symmetry_mean              569 non-null    float64\n",
      " 11  fractal_dimension_mean     569 non-null    float64\n",
      " 12  radius_ste                 569 non-null    float64\n",
      " 13  texture_ste                569 non-null    float64\n",
      " 14  perimeter_ste              569 non-null    float64\n",
      " 15  area_ste                   569 non-null    float64\n",
      " 16  smoothness_ste             569 non-null    float64\n",
      " 17  compactness_ste            569 non-null    float64\n",
      " 18  concavity_ste              569 non-null    float64\n",
      " 19  concave_poinits_ste        569 non-null    float64\n",
      " 20  symmetry_ste               569 non-null    float64\n",
      " 21  fractal_dimension_ste      569 non-null    float64\n",
      " 22  radius_largest             569 non-null    float64\n",
      " 23  texture_largest            569 non-null    float64\n",
      " 24  perimeter_largest          569 non-null    float64\n",
      " 25  area_largest               569 non-null    float64\n",
      " 26  smoothness_largest         569 non-null    float64\n",
      " 27  compactness_largest        569 non-null    float64\n",
      " 28  concavity_largest          569 non-null    float64\n",
      " 29  concave_poinits_largest    569 non-null    float64\n",
      " 30  symmetry_largest           569 non-null    float64\n",
      " 31  fractal_dimension_largest  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_Z1V6Dg-La_"
   },
   "source": [
    "Display the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.347349Z",
     "iopub.status.busy": "2022-09-29T01:22:04.347061Z",
     "iopub.status.idle": "2022-09-29T01:22:04.378177Z",
     "shell.execute_reply": "2022-09-29T01:22:04.377342Z"
    },
    "id": "hWxktwbv-KPp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_poinits_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_largest</th>\n",
       "      <th>texture_largest</th>\n",
       "      <th>perimeter_largest</th>\n",
       "      <th>area_largest</th>\n",
       "      <th>smoothness_largest</th>\n",
       "      <th>compactness_largest</th>\n",
       "      <th>concavity_largest</th>\n",
       "      <th>concave_poinits_largest</th>\n",
       "      <th>symmetry_largest</th>\n",
       "      <th>fractal_dimension_largest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave_poinits_mean  \\\n",
       "0            0.11840           0.27760         0.30010               0.14710   \n",
       "1            0.08474           0.07864         0.08690               0.07017   \n",
       "2            0.10960           0.15990         0.19740               0.12790   \n",
       "3            0.14250           0.28390         0.24140               0.10520   \n",
       "4            0.10030           0.13280         0.19800               0.10430   \n",
       "..               ...               ...             ...                   ...   \n",
       "564          0.11100           0.11590         0.24390               0.13890   \n",
       "565          0.09780           0.10340         0.14400               0.09791   \n",
       "566          0.08455           0.10230         0.09251               0.05302   \n",
       "567          0.11780           0.27700         0.35140               0.15200   \n",
       "568          0.05263           0.04362         0.00000               0.00000   \n",
       "\n",
       "     ...  radius_largest  texture_largest  perimeter_largest  area_largest  \\\n",
       "0    ...          25.380            17.33             184.60        2019.0   \n",
       "1    ...          24.990            23.41             158.80        1956.0   \n",
       "2    ...          23.570            25.53             152.50        1709.0   \n",
       "3    ...          14.910            26.50              98.87         567.7   \n",
       "4    ...          22.540            16.67             152.20        1575.0   \n",
       "..   ...             ...              ...                ...           ...   \n",
       "564  ...          25.450            26.40             166.10        2027.0   \n",
       "565  ...          23.690            38.25             155.00        1731.0   \n",
       "566  ...          18.980            34.12             126.70        1124.0   \n",
       "567  ...          25.740            39.42             184.60        1821.0   \n",
       "568  ...           9.456            30.37              59.16         268.6   \n",
       "\n",
       "     smoothness_largest  compactness_largest  concavity_largest  \\\n",
       "0               0.16220              0.66560             0.7119   \n",
       "1               0.12380              0.18660             0.2416   \n",
       "2               0.14440              0.42450             0.4504   \n",
       "3               0.20980              0.86630             0.6869   \n",
       "4               0.13740              0.20500             0.4000   \n",
       "..                  ...                  ...                ...   \n",
       "564             0.14100              0.21130             0.4107   \n",
       "565             0.11660              0.19220             0.3215   \n",
       "566             0.11390              0.30940             0.3403   \n",
       "567             0.16500              0.86810             0.9387   \n",
       "568             0.08996              0.06444             0.0000   \n",
       "\n",
       "     concave_poinits_largest  symmetry_largest  fractal_dimension_largest  \n",
       "0                     0.2654            0.4601                    0.11890  \n",
       "1                     0.1860            0.2750                    0.08902  \n",
       "2                     0.2430            0.3613                    0.08758  \n",
       "3                     0.2575            0.6638                    0.17300  \n",
       "4                     0.1625            0.2364                    0.07678  \n",
       "..                       ...               ...                        ...  \n",
       "564                   0.2216            0.2060                    0.07115  \n",
       "565                   0.1628            0.2572                    0.06637  \n",
       "566                   0.1418            0.2218                    0.07820  \n",
       "567                   0.2650            0.4087                    0.12400  \n",
       "568                   0.0000            0.2871                    0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4-Wn2jzVC1W"
   },
   "source": [
    "Split the dataset into training and test sets using [`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html){:.external}, [`pandas.DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html){:.external} and [`pandas.DataFrame.iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html){:.external}. Make sure to split the features from the target labels. The test set is used to evaluate your model's generalizability to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.382543Z",
     "iopub.status.busy": "2022-09-29T01:22:04.381870Z",
     "iopub.status.idle": "2022-09-29T01:22:04.386650Z",
     "shell.execute_reply": "2022-09-29T01:22:04.385763Z"
    },
    "id": "m2O60B-IVG9Q"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.390702Z",
     "iopub.status.busy": "2022-09-29T01:22:04.390041Z",
     "iopub.status.idle": "2022-09-29T01:22:04.394994Z",
     "shell.execute_reply": "2022-09-29T01:22:04.394181Z"
    },
    "id": "i06vHFv_QB24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.398415Z",
     "iopub.status.busy": "2022-09-29T01:22:04.398125Z",
     "iopub.status.idle": "2022-09-29T01:22:04.402524Z",
     "shell.execute_reply": "2022-09-29T01:22:04.401707Z"
    },
    "id": "19JaochhaQ3m"
   },
   "outputs": [],
   "source": [
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.406109Z",
     "iopub.status.busy": "2022-09-29T01:22:04.405670Z",
     "iopub.status.idle": "2022-09-29T01:22:04.410574Z",
     "shell.execute_reply": "2022-09-29T01:22:04.409759Z"
    },
    "id": "LmHRcbAfaSag"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.414470Z",
     "iopub.status.busy": "2022-09-29T01:22:04.413806Z",
     "iopub.status.idle": "2022-09-29T01:22:04.418699Z",
     "shell.execute_reply": "2022-09-29T01:22:04.417840Z"
    },
    "id": "w6JxBhBc_wwO"
   },
   "outputs": [],
   "source": [
    "# The `id` column can be dropped since each row is unique\n",
    "x_train, y_train = train_dataset.iloc[:, 2:], train_dataset.iloc[:, 1]\n",
    "x_test, y_test = test_dataset.iloc[:, 2:], test_dataset.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MWuJTKEDM-f"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "This dataset contains the mean, standard error, and largest values for each of the 10 tumor measurements collected per example. The `\"diagnosis\"` target column is a categorical variable with `'M'` indicating a malignant tumor and `'B'` indicating a benign tumor diagnosis. This column needs to be converted into a numerical binary format for model training.\n",
    "\n",
    "The [`pandas.Series.map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html){:.external} function is useful for mapping binary values to the categories.\n",
    "\n",
    "The dataset should also be converted to a tensor with the `tf.convert_to_tensor` function after the preprocessing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:04.422660Z",
     "iopub.status.busy": "2022-09-29T01:22:04.422345Z",
     "iopub.status.idle": "2022-09-29T01:22:07.586790Z",
     "shell.execute_reply": "2022-09-29T01:22:07.585719Z"
    },
    "id": "JEJHhN65a2VV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421    0\n",
      "47     1\n",
      "292    0\n",
      "186    1\n",
      "414    1\n",
      "      ..\n",
      "170    0\n",
      "401    0\n",
      "389    1\n",
      "525    0\n",
      "54     1\n",
      "Name: diagnosis, Length: 427, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = y_train.map({'B': 0, 'M': 1}), y_test.map({'B': 0, 'M': 1})\n",
    "print(y_train)\n",
    "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(427,), dtype=float32, numpy=\n",
       "array([0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ubs136WLNp"
   },
   "source": [
    "Use [`seaborn.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html){:.external} to review the joint distribution of a few pairs of mean-based features from the training set and observe how they relate to the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:07.591004Z",
     "iopub.status.busy": "2022-09-29T01:22:07.590752Z",
     "iopub.status.idle": "2022-09-29T01:22:12.696883Z",
     "shell.execute_reply": "2022-09-29T01:22:12.696095Z"
    },
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset.iloc[:, 1:6], hue = 'diagnosis', diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YOG5iKYKW_3"
   },
   "source": [
    "This pairplot demonstrates that certain features such as radius, perimeter and area are highly correlated. This is expected since the tumor radius is directly involved in the computation of both perimeter and area. Additionally, note that malignant diagnoses seem to be more right-skewed for many of the features.\n",
    "\n",
    "Make sure to also check the overall statistics. Note how each feature covers a vastly different range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:12.704322Z",
     "iopub.status.busy": "2022-09-29T01:22:12.703973Z",
     "iopub.status.idle": "2022-09-29T01:22:12.788162Z",
     "shell.execute_reply": "2022-09-29T01:22:12.787404Z"
    },
    "id": "yi2FzC3T21jR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>427.0</td>\n",
       "      <td>2.756014e+07</td>\n",
       "      <td>1.162735e+08</td>\n",
       "      <td>8670.00000</td>\n",
       "      <td>865427.500000</td>\n",
       "      <td>905539.00000</td>\n",
       "      <td>8.810829e+06</td>\n",
       "      <td>9.113205e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>1.414331e+01</td>\n",
       "      <td>3.528717e+00</td>\n",
       "      <td>6.98100</td>\n",
       "      <td>11.695000</td>\n",
       "      <td>13.43000</td>\n",
       "      <td>1.594000e+01</td>\n",
       "      <td>2.811000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>1.924468e+01</td>\n",
       "      <td>4.113131e+00</td>\n",
       "      <td>10.38000</td>\n",
       "      <td>16.330000</td>\n",
       "      <td>18.84000</td>\n",
       "      <td>2.168000e+01</td>\n",
       "      <td>3.381000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>9.206759e+01</td>\n",
       "      <td>2.431431e+01</td>\n",
       "      <td>43.79000</td>\n",
       "      <td>75.235000</td>\n",
       "      <td>86.87000</td>\n",
       "      <td>1.060000e+02</td>\n",
       "      <td>1.885000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>6.563190e+02</td>\n",
       "      <td>3.489106e+02</td>\n",
       "      <td>143.50000</td>\n",
       "      <td>420.050000</td>\n",
       "      <td>553.50000</td>\n",
       "      <td>7.908500e+02</td>\n",
       "      <td>2.499000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>9.633618e-02</td>\n",
       "      <td>1.436820e-02</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.085850</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>1.050000e-01</td>\n",
       "      <td>1.634000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>1.036597e-01</td>\n",
       "      <td>5.351893e-02</td>\n",
       "      <td>0.02344</td>\n",
       "      <td>0.063515</td>\n",
       "      <td>0.09182</td>\n",
       "      <td>1.296500e-01</td>\n",
       "      <td>3.454000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>8.833008e-02</td>\n",
       "      <td>7.965884e-02</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>1.297500e-01</td>\n",
       "      <td>4.268000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave_poinits_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>4.872688e-02</td>\n",
       "      <td>3.853594e-02</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>0.03390</td>\n",
       "      <td>7.409500e-02</td>\n",
       "      <td>2.012000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>427.0</td>\n",
       "      <td>1.804597e-01</td>\n",
       "      <td>2.637837e-02</td>\n",
       "      <td>0.12030</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.17840</td>\n",
       "      <td>1.947000e-01</td>\n",
       "      <td>2.906000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count          mean           std         min  \\\n",
       "id                    427.0  2.756014e+07  1.162735e+08  8670.00000   \n",
       "radius_mean           427.0  1.414331e+01  3.528717e+00     6.98100   \n",
       "texture_mean          427.0  1.924468e+01  4.113131e+00    10.38000   \n",
       "perimeter_mean        427.0  9.206759e+01  2.431431e+01    43.79000   \n",
       "area_mean             427.0  6.563190e+02  3.489106e+02   143.50000   \n",
       "smoothness_mean       427.0  9.633618e-02  1.436820e-02     0.05263   \n",
       "compactness_mean      427.0  1.036597e-01  5.351893e-02     0.02344   \n",
       "concavity_mean        427.0  8.833008e-02  7.965884e-02     0.00000   \n",
       "concave_poinits_mean  427.0  4.872688e-02  3.853594e-02     0.00000   \n",
       "symmetry_mean         427.0  1.804597e-01  2.637837e-02     0.12030   \n",
       "\n",
       "                                25%           50%           75%           max  \n",
       "id                    865427.500000  905539.00000  8.810829e+06  9.113205e+08  \n",
       "radius_mean               11.695000      13.43000  1.594000e+01  2.811000e+01  \n",
       "texture_mean              16.330000      18.84000  2.168000e+01  3.381000e+01  \n",
       "perimeter_mean            75.235000      86.87000  1.060000e+02  1.885000e+02  \n",
       "area_mean                420.050000     553.50000  7.908500e+02  2.499000e+03  \n",
       "smoothness_mean            0.085850       0.09566  1.050000e-01  1.634000e-01  \n",
       "compactness_mean           0.063515       0.09182  1.296500e-01  3.454000e-01  \n",
       "concavity_mean             0.029570       0.05999  1.297500e-01  4.268000e-01  \n",
       "concave_poinits_mean       0.019650       0.03390  7.409500e-02  2.012000e-01  \n",
       "symmetry_mean              0.161700       0.17840  1.947000e-01  2.906000e-01  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe().transpose()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8pDCIFjMla8"
   },
   "source": [
    "Given the inconsistent ranges, it is beneficial to standardize the data such that each feature has a zero mean and unit variance. This process is called [normalization](https://developers.google.com/machine-learning/glossary#normalization){:.external}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:12.792117Z",
     "iopub.status.busy": "2022-09-29T01:22:12.791858Z",
     "iopub.status.idle": "2022-09-29T01:22:12.812778Z",
     "shell.execute_reply": "2022-09-29T01:22:12.812045Z"
    },
    "id": "FrzKNFNjLQDl"
   },
   "outputs": [],
   "source": [
    "class Normalize(tf.Module):\n",
    "  def __init__(self, x):\n",
    "    # Initialize the mean and standard deviation for normalization\n",
    "    self.mean = tf.Variable(tf.math.reduce_mean(x, axis=0))\n",
    "    self.std = tf.Variable(tf.math.reduce_std(x, axis=0))\n",
    "\n",
    "  def norm(self, x):\n",
    "    # Normalize the input\n",
    "    return (x - self.mean)/self.std\n",
    "\n",
    "  def unnorm(self, x):\n",
    "    # Unnormalize the input\n",
    "    return (x * self.std) + self.mean\n",
    "\n",
    "norm_x = Normalize(x_train)\n",
    "x_train_norm, x_test_norm = norm_x.norm(x_train), norm_x.norm(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o3CrycBXA2s"
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Before building a logistic regression model, it is crucial to understand the method's differences compared to traditional linear regression.\n",
    "\n",
    "### Logistic regression fundamentals\n",
    "\n",
    "Linear regression returns a linear combination of its inputs; this output is unbounded. The output of a [logistic regression](https://developers.google.com/machine-learning/glossary#logistic_regression){:.external} is in the `(0, 1)` range. For each example, it represents the probability that the example belongs to the _positive_ class.\n",
    "\n",
    "Logistic regression maps the continuous outputs of traditional linear regression, `(-∞, ∞)`, to probabilities, `(0, 1)`. This transformation is also symmetric so that flipping the sign of the linear output results in the inverse of the original probability.\n",
    "\n",
    "Let $Y$ denote the probability of being in class `1` (the tumor is malignant). The desired mapping can be achieved by interpreting the linear regression output as the [log odds](https://developers.google.com/machine-learning/glossary#log-odds){:.external} ratio of being in class `1` as opposed to class `0`:\n",
    "\n",
    "$$\\ln(\\frac{Y}{1-Y}) = wX + b$$\n",
    "\n",
    "By setting $wX + b = z$, this equation can then be solved for $Y$:\n",
    "\n",
    "$$Y = \\frac{e^{z}}{1 + e^{z}} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The expression $\\frac{1}{1 + e^{-z}}$ is known as the [sigmoid function](https://developers.google.com/machine-learning/glossary#sigmoid_function){:.external} $\\sigma(z)$. Hence, the equation for logistic regression can be written as $Y = \\sigma(wX + b)$.\n",
    "\n",
    "The dataset in this tutorial deals with a high-dimensional feature matrix. Therefore, the above equation must be rewritten in a matrix vector form as follows:\n",
    "\n",
    "$${\\mathrm{Y}} = \\sigma({\\mathrm{X}}w + b)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\underset{m\\times 1}{\\mathrm{Y}}$: a target vector\n",
    "* $\\underset{m\\times n}{\\mathrm{X}}$: a feature matrix\n",
    "* $\\underset{n\\times 1}w$: a weight vector\n",
    "* $b$: a bias\n",
    "* $\\sigma$: a sigmoid function applied to each element of the output vector\n",
    "\n",
    "Start by visualizing the sigmoid function, which transforms the linear output, `(-∞, ∞)`, to fall between `0` and `1`. The sigmoid function is available in `tf.math.sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:12.817128Z",
     "iopub.status.busy": "2022-09-29T01:22:12.816618Z",
     "iopub.status.idle": "2022-09-29T01:22:13.087416Z",
     "shell.execute_reply": "2022-09-29T01:22:13.086617Z"
    },
    "id": "ThHaV_RmucZl"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-10, 10, 500)\n",
    "x = tf.cast(x, tf.float32)\n",
    "f = lambda x : (1/20)*x + 0.6\n",
    "plt.plot(x, tf.math.sigmoid(x))\n",
    "plt.ylim((-0.1,1.1))\n",
    "plt.title(\"Sigmoid function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMXEhrZuKECV"
   },
   "source": [
    "### The log loss function\n",
    "\n",
    "The  [log loss](https://developers.google.com/machine-learning/glossary#Log_Loss){:.external}, or binary cross-entropy loss, is the ideal loss function for a binary classification problem with logistic regression. For each example, the log loss quantifies the similarity between a predicted probability and the example's true value. It is determined by the following equation:\n",
    "\n",
    "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\hat{y}_i) + (1- y_i)\\cdot\\log(1 - \\hat{y}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$: a vector of predicted probabilities\n",
    "* $y$: a vector of true targets\n",
    "\n",
    "You can use the `tf.nn.sigmoid_cross_entropy_with_logits` function to compute the log loss. This function automatically applies the sigmoid activation to the regression output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:13.091720Z",
     "iopub.status.busy": "2022-09-29T01:22:13.091435Z",
     "iopub.status.idle": "2022-09-29T01:22:13.095810Z",
     "shell.execute_reply": "2022-09-29T01:22:13.095013Z"
    },
    "id": "JVBInnSqS36W"
   },
   "outputs": [],
   "source": [
    "def log_loss(y_pred, y):\n",
    "  # Compute the log loss function\n",
    "  ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
    "  return tf.reduce_mean(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_mutLj0KNUb"
   },
   "source": [
    "### The gradient descent update rule\n",
    "\n",
    "The TensorFlow Core APIs support automatic differentiation with `tf.GradientTape`. If you are curious about the mathematics behind the logistic regression [gradient updates](https://developers.google.com/machine-learning/glossary#gradient_descent){:.external}, here is a short explanation:\n",
    "\n",
    "In the above equation for the log loss, recall that each $\\hat{y}_i$ can be rewritten in terms of the inputs as $\\sigma({\\mathrm{X_i}}w + b)$.\n",
    "\n",
    "The goal is to find a $w^*$ and $b^*$ that minimize the log loss:\n",
    "\n",
    "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\cdot\\log(\\sigma({\\mathrm{X_i}}w + b)) + (1- y_i)\\cdot\\log(1 - \\sigma({\\mathrm{X_i}}w + b))$$\n",
    "\n",
    "By taking the gradient $L$ with respect to $w$, you get the following:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m}(\\sigma({\\mathrm{X}}w + b) - y)X$$\n",
    "\n",
    "By taking the gradient $L$ with respect to $b$, you get the following:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}\\sigma({\\mathrm{X_i}}w + b) - y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTCndUecKZho"
   },
   "source": [
    "Now, build the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:13.100003Z",
     "iopub.status.busy": "2022-09-29T01:22:13.099746Z",
     "iopub.status.idle": "2022-09-29T01:22:13.105512Z",
     "shell.execute_reply": "2022-09-29T01:22:13.104772Z"
    },
    "id": "c0sXM7qLlKfZ"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.built = False\n",
    "    \n",
    "  def __call__(self, x, train=True):\n",
    "    # Initialize the model parameters on the first call\n",
    "    if not self.built:\n",
    "      # Randomly generate the weights and the bias term\n",
    "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
    "      rand_b = tf.random.uniform(shape=[], seed=22)\n",
    "      self.w = tf.Variable(rand_w)\n",
    "      self.b = tf.Variable(rand_b)\n",
    "      self.built = True\n",
    "    # Compute the model output\n",
    "    z = tf.add(tf.matmul(x, self.w), self.b)\n",
    "    z = tf.squeeze(z, axis=1)\n",
    "    if train:\n",
    "      return z\n",
    "    return tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eObQu9fDnXGL"
   },
   "source": [
    "To validate, make sure the untrained model outputs values in the range of `(0, 1)` for a small subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:13.109826Z",
     "iopub.status.busy": "2022-09-29T01:22:13.109318Z",
     "iopub.status.idle": "2022-09-29T01:22:13.112805Z",
     "shell.execute_reply": "2022-09-29T01:22:13.112086Z"
    },
    "id": "5bIovC0Z4QHJ"
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:13.116250Z",
     "iopub.status.busy": "2022-09-29T01:22:13.115985Z",
     "iopub.status.idle": "2022-09-29T01:22:14.611671Z",
     "shell.execute_reply": "2022-09-29T01:22:14.610682Z"
    },
    "id": "QJ2ievISyf0p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9994984 , 0.9978607 , 0.29620063, 0.01979047, 0.33149233],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_reg(x_train_norm[:5], train=False)\n",
    "y_pred.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PribnwDHUksC"
   },
   "source": [
    "Next, write an accuracy function to calculate the proportion of correct classifications during training. In order to retrieve the classifications from the predicted probabilities, set a threshold for which all probabilities higher than the threshold belong to class `1`. This is a configurable hyperparameter that can be set to `0.5` as a default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:14.616045Z",
     "iopub.status.busy": "2022-09-29T01:22:14.615760Z",
     "iopub.status.idle": "2022-09-29T01:22:14.622687Z",
     "shell.execute_reply": "2022-09-29T01:22:14.621942Z"
    },
    "id": "ssnVcKg7oMe6"
   },
   "outputs": [],
   "source": [
    "def predict_class(y_pred, thresh=0.5):\n",
    "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
    "  return tf.cast(y_pred > thresh, tf.float32)\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "  # Return the proportion of matches between `y_pred` and `y`\n",
    "  y_pred = tf.math.sigmoid(y_pred)\n",
    "  y_pred_class = predict_class(y_pred)\n",
    "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
    "  acc_val = tf.reduce_mean(check_equal)\n",
    "  return acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_0KHQ25_2dF"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Using mini-batches for training provides both memory efficiency and faster convergence. The `tf.data.Dataset` API has useful functions for batching and shuffling. The API enables you to build complex input pipelines from simple, reusable pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:14.627066Z",
     "iopub.status.busy": "2022-09-29T01:22:14.626808Z",
     "iopub.status.idle": "2022-09-29T01:22:14.646415Z",
     "shell.execute_reply": "2022-09-29T01:22:14.645585Z"
    },
    "id": "vJD7-4U0etqa"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(427,), dtype=float32, numpy=\n",
       "array([0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLiWZZPBSDip"
   },
   "source": [
    "Now write a training loop for the logistic regression model. The loop utilizes the log loss function and its gradients with respect to the input in order to iteratively update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:14.650470Z",
     "iopub.status.busy": "2022-09-29T01:22:14.650170Z",
     "iopub.status.idle": "2022-09-29T01:22:29.137794Z",
     "shell.execute_reply": "2022-09-29T01:22:29.136913Z"
    },
    "id": "jNC3D1DGsGgK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training log loss: 0.661\n",
      "Epoch: 20, Training log loss: 0.418\n",
      "Epoch: 40, Training log loss: 0.269\n",
      "Epoch: 60, Training log loss: 0.178\n",
      "Epoch: 80, Training log loss: 0.137\n",
      "Epoch: 100, Training log loss: 0.116\n",
      "Epoch: 120, Training log loss: 0.106\n",
      "Epoch: 140, Training log loss: 0.096\n",
      "Epoch: 160, Training log loss: 0.094\n",
      "Epoch: 180, Training log loss: 0.089\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "# Set up the training loop and begin training\n",
    "for epoch in range(epochs):\n",
    "  batch_losses_train, batch_accs_train = [], []\n",
    "  batch_losses_test, batch_accs_test = [], []\n",
    "\n",
    "  # Iterate over the training data\n",
    "  for x_batch, y_batch in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred_batch = log_reg(x_batch)\n",
    "      batch_loss = log_loss(y_pred_batch, y_batch)\n",
    "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
    "    # Update the parameters with respect to the gradient calculations\n",
    "    grads = tape.gradient(batch_loss, log_reg.variables)\n",
    "    for g,v in zip(grads, log_reg.variables):\n",
    "      v.assign_sub(learning_rate * g)\n",
    "    # Keep track of batch-level training performance\n",
    "    batch_losses_train.append(batch_loss)\n",
    "    batch_accs_train.append(batch_acc)\n",
    "\n",
    "  # Iterate over the testing data\n",
    "  for x_batch, y_batch in test_dataset:\n",
    "    y_pred_batch = log_reg(x_batch)\n",
    "    batch_loss = log_loss(y_pred_batch, y_batch)\n",
    "    batch_acc = accuracy(y_pred_batch, y_batch)\n",
    "    # Keep track of batch-level testing performance\n",
    "    batch_losses_test.append(batch_loss)\n",
    "    batch_accs_test.append(batch_acc)\n",
    "\n",
    "  # Keep track of epoch-level model performance\n",
    "  train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
    "  test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
    "  train_losses.append(train_loss)\n",
    "  train_accs.append(train_acc)\n",
    "  test_losses.append(test_loss)\n",
    "  test_accs.append(test_acc)\n",
    "  if epoch % 20 == 0:\n",
    "    print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoLiAg7fYft7"
   },
   "source": [
    "### Performance evaluation\n",
    "\n",
    "Observe the changes in your model's loss and accuracy over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.142567Z",
     "iopub.status.busy": "2022-09-29T01:22:29.142187Z",
     "iopub.status.idle": "2022-09-29T01:22:29.418720Z",
     "shell.execute_reply": "2022-09-29T01:22:29.417798Z"
    },
    "id": "mv3oCQPvWhr0"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
    "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Log loss vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.422667Z",
     "iopub.status.busy": "2022-09-29T01:22:29.422376Z",
     "iopub.status.idle": "2022-09-29T01:22:29.704965Z",
     "shell.execute_reply": "2022-09-29T01:22:29.704138Z"
    },
    "id": "D2HDVGLPODIE"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
    "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.708671Z",
     "iopub.status.busy": "2022-09-29T01:22:29.708268Z",
     "iopub.status.idle": "2022-09-29T01:22:29.712738Z",
     "shell.execute_reply": "2022-09-29T01:22:29.711918Z"
    },
    "id": "jonKhUzuPyfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training log loss: 0.089\n",
      "Final testing log Loss: 0.077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
    "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.716158Z",
     "iopub.status.busy": "2022-09-29T01:22:29.715877Z",
     "iopub.status.idle": "2022-09-29T01:22:29.720263Z",
     "shell.execute_reply": "2022-09-29T01:22:29.719498Z"
    },
    "id": "d3DF4qyrPyke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy: 0.968\n",
      "Final testing accuracy: 0.979\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
    "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrj1TbOJasjA"
   },
   "source": [
    "The model demonstrates a high accuracy and a low loss when it comes to classifying tumors in the training dataset and also generalizes well to the unseen test data. To go one step further, you can explore error rates that give more insight beyond the overall accuracy score. The two most popular error rates for binary classification problems are the false positive rate (FPR) and the false negative rate (FNR).\n",
    "\n",
    "For this problem, the FPR is the proportion of malignant tumor predictions amongst tumors that are actually benign. Conversely, the FNR is the proportion of benign tumor predictions among tumors that are actually malignant.\n",
    "\n",
    "Compute a confusion matrix using [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix){:.external}, which evaluates the accuracy of the classification, and use matplotlib to display the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.724245Z",
     "iopub.status.busy": "2022-09-29T01:22:29.723994Z",
     "iopub.status.idle": "2022-09-29T01:22:29.732217Z",
     "shell.execute_reply": "2022-09-29T01:22:29.731464Z"
    },
    "id": "OJO7YkA8ZDMU"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y, y_classes, typ):\n",
    "  # Compute the confusion matrix and normalize it\n",
    "  plt.figure(figsize=(10,10))\n",
    "  confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
    "  confusion_normalized = confusion / confusion.sum(axis=1)\n",
    "  axis_labels = range(2)\n",
    "  ax = sns.heatmap(\n",
    "      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "      cmap='Blues', annot=True, fmt='.4f', square=True)\n",
    "  plt.title(f\"Confusion matrix: {typ}\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "\n",
    "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
    "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.735985Z",
     "iopub.status.busy": "2022-09-29T01:22:29.735381Z",
     "iopub.status.idle": "2022-09-29T01:22:29.954945Z",
     "shell.execute_reply": "2022-09-29T01:22:29.954025Z"
    },
    "id": "OQ5DFcleiDFm"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_train, train_classes, 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:29.958846Z",
     "iopub.status.busy": "2022-09-29T01:22:29.958254Z",
     "iopub.status.idle": "2022-09-29T01:22:30.177265Z",
     "shell.execute_reply": "2022-09-29T01:22:30.176390Z"
    },
    "id": "gtfcsAp_iCNR"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, test_classes, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlivxaDmTnGq"
   },
   "source": [
    "Observe the error rate measurements and interpret their significance in the context of this example. In many medical testing studies such as cancer detection, having a high false positive rate to ensure a low false negative rate is perfectly acceptable and in fact encouraged since the risk of missing a malignant tumor diagnosis (false negative) is a lot worse than misclassifying a benign tumor as malignant (false positive).\n",
    "\n",
    "In order to control for the FPR and FNR, try changing the threshold hyperparameter before classifying the probability predictions. A lower threshold increases the model's overall chances of making a malignant tumor classification. This inevitably increases the number of false positives and the FPR but it also helps to decrease the number of false negatives and the FNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ADEN2rb4Nhj"
   },
   "source": [
    "## Save the model\n",
    "\n",
    "Start by making an export module that takes in raw data and performs the following operations:\n",
    "- Normalization\n",
    "- Probability prediction\n",
    "- Class prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:30.181687Z",
     "iopub.status.busy": "2022-09-29T01:22:30.181390Z",
     "iopub.status.idle": "2022-09-29T01:22:30.187122Z",
     "shell.execute_reply": "2022-09-29T01:22:30.186328Z"
    },
    "id": "6KPRHCzg4ZxH"
   },
   "outputs": [],
   "source": [
    "class ExportModule(tf.Module):\n",
    "  def __init__(self, model, norm_x, class_pred):\n",
    "    # Initialize pre- and post-processing functions\n",
    "    self.model = model\n",
    "    self.norm_x = norm_x\n",
    "    self.class_pred = class_pred\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)])\n",
    "  def __call__(self, x):\n",
    "    # Run the `ExportModule` for new data points\n",
    "    x = self.norm_x.norm(x)\n",
    "    y = self.model(x, train=False)\n",
    "    y = self.class_pred(y)\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:30.190768Z",
     "iopub.status.busy": "2022-09-29T01:22:30.190519Z",
     "iopub.status.idle": "2022-09-29T01:22:30.194340Z",
     "shell.execute_reply": "2022-09-29T01:22:30.193584Z"
    },
    "id": "2YzRclo5-yjO"
   },
   "outputs": [],
   "source": [
    "log_reg_export = ExportModule(model=log_reg,\n",
    "                              norm_x=norm_x,\n",
    "                              class_pred=predict_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtofGIBN_qFd"
   },
   "source": [
    "If you want to save the model at its current state, you can do so with the `tf.saved_model.save` function. To load a saved model and make predictions, use the `tf.saved_model.load` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:30.198623Z",
     "iopub.status.busy": "2022-09-29T01:22:30.198029Z",
     "iopub.status.idle": "2022-09-29T01:22:30.505111Z",
     "shell.execute_reply": "2022-09-29T01:22:30.504388Z"
    },
    "id": "a4Qum1Ts_pmF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\MARKEL~1.NIK\\AppData\\Local\\Temp\\tmp1_a6_mnu\\log_reg_export\\assets\n"
     ]
    }
   ],
   "source": [
    "models = tempfile.mkdtemp()\n",
    "save_path = os.path.join(models, 'log_reg_export')\n",
    "tf.saved_model.save(log_reg_export, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T01:22:30.509218Z",
     "iopub.status.busy": "2022-09-29T01:22:30.508970Z",
     "iopub.status.idle": "2022-09-29T01:22:30.572286Z",
     "shell.execute_reply": "2022-09-29T01:22:30.571580Z"
    },
    "id": "3KPILr1i_M_c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_loaded = tf.saved_model.load(save_path)\n",
    "test_preds = log_reg_loaded(x_test)\n",
    "test_preds[:10].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook introduced a few techniques to handle a logistic regression problem. Here are a few more tips that may help:\n",
    "\n",
    "- The [TensorFlow Core APIs](https://www.tensorflow.org/guide/core) can be used to build machine learning workflows with high levels of configurability\n",
    "- Analyzing error rates is a great way to gain more insight about a classification model's performance beyond its overall accuracy score.\n",
    "- Overfitting is another common problem for logistic regression models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](../../tutorials/keras/overfit_and_underfit.ipynb) tutorial for more help with this.\n",
    "\n",
    "For more examples of using the TensorFlow Core APIs, check out the [guide](https://www.tensorflow.org/guide/core). If you want to learn more about loading and preparing data, see the tutorials on [image data loading](../../load_data/images.ipynb) or [CSV data loading](../../load_data/csv.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "logistic_regression_core.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('thisone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7996a5cd28996b69f6427e562b95ceb603a092331acdb6a620e160fb7b86c167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
